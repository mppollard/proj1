{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data from Landing to Raw\n",
    "This notebook preprocesses the landing data, enacting basic data transformation such as column renaming and data type conversion\n",
    "### Load constants and libraries\n",
    "First of all we load any constant values for use within the notebook, and load any libraries required. , and initiate a spark object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all constants used in the note books\n",
    "from constants import *\n",
    "\n",
    "# libraries required\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/08/16 11:53:56 WARN Utils: Your hostname, PVM203L resolves to a loopback address: 127.0.1.1; using 172.20.203.188 instead (on interface eth0)\n",
      "23/08/16 11:53:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/16 11:53:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.driver.memory', '4g')\n",
    "    .config('spark.executor.memory', '2g')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the TLC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_raw_tlc_data(source: str, use_schema: str, rename_cols: dict[str:str]) -> None:\n",
    "    '''\n",
    "    Retrieves downloaded TLC data (landing data), applies the schema of a selected month of data (use_schema)\n",
    "    to all other months in the dataset, to ensure consistency, and renames columns of interest. \n",
    "    Saves this raw data down accordingly\n",
    "    Arguments:\n",
    "        source = TLC data source (yellow, green, or fhvhv)\n",
    "        use_schema = YYYY-MM to use as default schema for all months\n",
    "        rename_cols = columns of interest to be renamed\n",
    "                      dict with keys: original column name, and values: new column name\n",
    "    Ouput: None\n",
    "    '''\n",
    "    # get the schema of the supplied year-month\n",
    "    get_schema = spark.read.parquet(LANDING_DATA + source +'/' + use_schema + '.parquet').schema\n",
    "    # cycle through the landing data and save down with columns cast to correct types\n",
    "    for file in os.listdir(LANDING_DATA+source):\n",
    "        month_data = spark.read.parquet(LANDING_DATA + source +'/' + file)\n",
    "        month_data = month_data \\\n",
    "            .select([F.col(c).cast(get_schema[i].dataType) for i, c in enumerate(month_data.columns)])\n",
    "        # rename columns if they exist in rename_cols, otherwise just keep original (in lower case)\n",
    "        month_data = month_data.select([F.col(c).alias(rename_cols.get(c, c.lower())) for c in month_data.columns])\n",
    "        # save the raw data\n",
    "        month_data.write.mode('overwrite').parquet(RAW_DATA+source+'/'+file.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving yellow data\n",
      "Saving green data\n",
      "Saving fhvhv data\n"
     ]
    }
   ],
   "source": [
    "# prepare the raw data for each of the yellow, green, and fhv data sets in turn\n",
    "print(f'Saving yellow data')\n",
    "rename_cols = {'tpep_pickup_datetime': 'pickup_datetime', 'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "               'trip_distance': 'trip_distance', 'PULocationID':'pickup_id','fare_amount':'fare'}\n",
    "prepare_raw_tlc_data(source='yellow', use_schema='2023-02', rename_cols=rename_cols)\n",
    "\n",
    "print(f'Saving green data')\n",
    "rename_cols = {'lpep_pickup_datetime': 'pickup_datetime', 'lpep_dropoff_datetime': 'dropoff_datetime',\n",
    "               'trip_distance': 'trip_distance', 'PULocationID':'pickup_id','fare_amount':'fare'}\n",
    "prepare_raw_tlc_data(source='green', use_schema='2023-02', rename_cols=rename_cols)\n",
    "\n",
    "print(f'Saving fhvhv data')\n",
    "rename_cols = {'Pickup_datetime': 'pickup_datetime', 'DropOff_datetime': 'dropoff_datetime',\n",
    "               'trip_miles': 'trip_distance', 'PULocationID':'pickup_id','base_passenger_fare':'fare'}\n",
    "prepare_raw_tlc_data(source='fhvhv', use_schema='2023-02', rename_cols=rename_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isfloat(value): \n",
    "    ''' \n",
    "    A helper function to determine if value, which is a string, can be converted to float or not\n",
    "    '''    \n",
    "    try: \n",
    "        return True, float(value) \n",
    "    except ValueError: \n",
    "        return False, value \n",
    "\n",
    "def fill_missing(all_data: pd.DataFrame, fill_col: str) -> None:\n",
    "    '''\n",
    "    Converts a column of strings to numeric, and imputes missing values. This is for timeseries values,\n",
    "    therefore the method of imputation is to take an average of adjacent values if available, or just the\n",
    "    preceding and next value if only one of these is avialable    \n",
    "    Arguments:\n",
    "        all_data = DataFrame to execute on\n",
    "        fill_col = column to execute on\n",
    "    Ouput: None\n",
    "    '''\n",
    "    \n",
    "    # Extract the column being imputed\n",
    "    data = all_data[fill_col]\n",
    "    \n",
    "    # find first valid entry, this is used to fill entries at the beginning of the series\n",
    "    for row, entry in enumerate(data):\n",
    "        found_float, first_float = isfloat(entry)\n",
    "        if found_float:\n",
    "            break\n",
    "    \n",
    "    # if no numeric values are found we cannot run imputation\n",
    "    if not(found_float):\n",
    "        print('no numeric values found')\n",
    "        return\n",
    "\n",
    "    # iterate through the data and apply the imputation methodology described in the docstring\n",
    "    for row, entry in enumerate(data):\n",
    "        if not(isfloat(entry)[0]):\n",
    "            found_prior, prior_float = isfloat(data[max(row-1,0)])\n",
    "            found_next, next_float = isfloat(data[min(row+1,len(data))])\n",
    "            \n",
    "            if not(found_prior) and not(found_next):\n",
    "                all_data.loc[row, fill_col] = first_float\n",
    "            elif not(found_prior) and found_next:\n",
    "                all_data.loc[row, fill_col] = next_float\n",
    "            elif found_prior and not(found_next):\n",
    "                all_data.loc[row, fill_col] = prior_float\n",
    "            else:\n",
    "                all_data.loc[row, fill_col] = (prior_float+next_float)/2\n",
    "    print(f'data for {fill_col} successfully filled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data for sped successfully filled\n",
      "data for p01m successfully filled\n"
     ]
    }
   ],
   "source": [
    "# speed and precipitation data requries imputation where dummy values have been included\n",
    "weather_data = pd.read_csv(f'{LANDING_DATA}JFK.csv', sep=',')\n",
    "fill_missing(weather_data, 'sped')\n",
    "fill_missing(weather_data, 'p01m')\n",
    "\n",
    "# Data type conversion\n",
    "weather_data[['sped', 'p01m']] = weather_data[['sped', 'p01m']].apply(pd.to_numeric)\n",
    "weather_data['valid'] = weather_data['valid'].apply(pd.to_datetime)\n",
    "\n",
    "# pickle data format is implemented to preserve the schema\n",
    "weather_data.to_pickle(f'{RAW_DATA}weather_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Events Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_data = pd.read_csv(f'{LANDING_DATA}events_data.csv')\n",
    "\n",
    "# Data type conversion\n",
    "events_data['create_time']=events_data['create_time'].apply(pd.to_datetime)\n",
    "events_data['close_time']=events_data['close_time'].apply(pd.to_datetime)\n",
    "\n",
    "# pickle data format is implemented to preserve the schema\n",
    "events_data.to_pickle(f'{RAW_DATA}events_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
