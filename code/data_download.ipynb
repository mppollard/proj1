{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TLC_URL = 'https://d37ci6vzurychx.cloudfront.net/trip-data/'\n",
    "EVENT_URL = 'https://data.cityofnewyork.us/api/views/6v4b-5gp4/rows.csv?accessType=DOWNLOAD&bom=true&format=true&delimiter=%3B'\n",
    "\n",
    "LANDING_DATA = '../data/landing/'\n",
    "ALL_SOURCES = ('yellow', 'green', 'fhvhv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# from the current `proj1` directory\n",
    "output_relative_dir = '../data/'\n",
    "\n",
    "# check if it exists as it makedir will raise an error if it does exist\n",
    "if not os.path.exists(output_relative_dir):\n",
    "    os.makedirs(output_relative_dir)\n",
    "    \n",
    "# now, for each type of data set we will need, we will create the paths\n",
    "for target_dir in ('landing', 'raw', 'curated', 'analysis'): \n",
    "    if not os.path.exists(output_relative_dir + target_dir):\n",
    "        os.makedirs(output_relative_dir + target_dir)\n",
    "\n",
    "output_relative_dir = '../data/landing/'\n",
    "for target_dir in ALL_SOURCES: \n",
    "    if not os.path.exists(output_relative_dir + target_dir):\n",
    "        os.makedirs(output_relative_dir + target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "def retrieve_data(source: str, year: str, month: str) -> None:\n",
    "    # generate url\n",
    "    get_url = f'{TLC_URL+source+\"_tripdata_\"}{year}-{month}.parquet'\n",
    "    # generate output location and filename\n",
    "    get_path = f'{LANDING_DATA+source+\"/\"}/{year}-{month}.parquet'\n",
    "    # download\n",
    "    urlretrieve(get_url, get_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin year 2022 month 01\n",
      "Completed year 2022 month 01\n",
      "Begin year 2022 month 02\n",
      "Completed year 2022 month 02\n",
      "Begin year 2022 month 03\n",
      "Completed year 2022 month 03\n",
      "Begin year 2022 month 04\n",
      "Completed year 2022 month 04\n",
      "Begin year 2022 month 05\n",
      "Completed year 2022 month 05\n",
      "Begin year 2022 month 06\n",
      "Completed year 2022 month 06\n",
      "Begin year 2022 month 07\n",
      "Completed year 2022 month 07\n",
      "Begin year 2022 month 08\n",
      "Completed year 2022 month 08\n",
      "Begin year 2022 month 09\n",
      "Completed year 2022 month 09\n",
      "Begin year 2022 month 10\n",
      "Completed year 2022 month 10\n",
      "Begin year 2022 month 11\n",
      "Completed year 2022 month 11\n",
      "Begin year 2022 month 12\n",
      "Completed year 2022 month 12\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# take all data from May 2020 - May 2023\n",
    "years = ['2020', '2021', '2022', '2023']\n",
    "start_year = '2020'; start_month = 5\n",
    "end_year = '2023'; end_month = 5\n",
    "months = range(1, 13)\n",
    "\n",
    "all_months = itertools.product(years, months)\n",
    "for year_month in all_months:\n",
    "    year, month = year_month\n",
    "    if not((year == start_year and month < start_month) | (year == end_year and month > end_month)):\n",
    "        # 0-fill i.e 1 -> 01, 2 -> 02, etc\n",
    "        month = str(month).zfill(2) \n",
    "        print(f\"Begin year {year} month {month}\")\n",
    "        for source in ALL_SOURCES:\n",
    "            retrieve_data(source, year, month)        \n",
    "        print(f\"Completed year {year} month {month}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/landing/Parks_Special_Events.csv',\n",
       " <http.client.HTTPMessage at 0x7faccaf55de0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "# generate output location and filename\n",
    "get_path = f'{LANDING_DATA}Parks_Special_Events.csv'\n",
    "# download\n",
    "urlretrieve(EVENT_URL, get_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 6, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m event_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mLANDING_DATA\u001b[39m}\u001b[39;49;00m\u001b[39mParks_Special_Events.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m         nrows\n\u001b[1;32m   1706\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 6, saw 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "event_data = pd.read_csv(f'{LANDING_DATA}Parks_Special_Events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def fetch(station_id):\n",
    "    \"\"\"Download data we are interested in!\"\"\"\n",
    "    localfn = f\"{station_id}.csv\"\n",
    "    if os.path.isfile(localfn):\n",
    "        print(f\"- Cowardly refusing to over-write existing file: {localfn}\")\n",
    "        return\n",
    "    print(f\"+ Downloading for {station_id}\")\n",
    "    enddt = date.today() + timedelta(days=2)\n",
    "    uri = (\n",
    "        \"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "        f\"station={station_id}&data=tmpc&year1=2021&month1=5&day1=1&\"\n",
    "        f\"year2=2023&month2=5&day2=31&\"\n",
    "        \"tz=Etc%2FUTC&format=onlycomma&latlon=no&elev=no&missing=M&trace=T&\"\n",
    "        \"direct=yes&report_type=3\"\n",
    "    )\n",
    "    res = requests.get(uri, timeout=300)\n",
    "    with open(localfn, \"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(res.text)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main loop.\"\"\"\n",
    "    # Step 1: Fetch global METAR geojson metadata\n",
    "    # https://mesonet.agron.iastate.edu/sites/networks.php\n",
    "    req = requests.get(\n",
    "        \"http://mesonet.agron.iastate.edu/geojson/network/AZOS.geojson\",\n",
    "        timeout=60,\n",
    "    )\n",
    "    geojson = req.json()\n",
    "    for feature in geojson[\"features\"]:\n",
    "        station_id = feature[\"id\"]\n",
    "        props = feature[\"properties\"]\n",
    "        # We want stations with data to today (archive_end is null)\n",
    "        if props[\"archive_end\"] is None:\n",
    "            continue\n",
    "        # We want stations with data to at least 1943\n",
    "        if props[\"archive_begin\"] is None:\n",
    "            continue\n",
    "        archive_begin = datetime.strptime(props[\"archive_begin\"], \"%Y-%m-%d\")\n",
    "        if archive_begin.year > 1943:\n",
    "            continue\n",
    "        # Horray, fetch data\n",
    "        fetch(station_id)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
